{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 Decision Tree, Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import numpy as np\n",
    "from scipy.stats import entropy, zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingHouseDf = pd.read_csv(r\"housing_train.txt\", header=None, delim_whitespace=True)\n",
    "testingHouseDf = pd.read_csv(r\"housing_test.txt\", header=None, delim_whitespace=True)\n",
    "\n",
    "spambaseDf = pd.read_csv(r\"spambase.data\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
      "\n",
      "       11    12    13  \n",
      "0  396.90  4.98  24.0  \n",
      "1  396.90  9.14  21.6  \n",
      "2  392.83  4.03  34.7  \n",
      "3  394.63  2.94  33.4  \n",
      "4  396.90  5.33  36.2  \n",
      "======\n",
      "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
      "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
      "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
      "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
      "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
      "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
      "\n",
      "      49   50     51     52     53     54   55    56  57  \n",
      "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
      "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
      "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
      "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
      "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "print(trainingHouseDf.head())\n",
    "print (\"======\")\n",
    "print(spambaseDf.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM 1 [50 points]**\n",
    "\n",
    "Using each dataset, build a decision tree (or regression tree) from the training set. Since the features are numeric values, you will need to use thresholds mechanisms. Report (txt or pdf file) for each dataset the training and testing error for each of your trials:\n",
    "* simple decision tree using something like Information Gain or other Entropy-like notion of randomness\n",
    "* regression tree\n",
    "* try to limit the size of the tree to get comparable training and testing errors (avoid overfitting typical of deep trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-folds for spambase\n",
    "\n",
    "foldSize = int(len(spambaseDf)/10)\n",
    "spambaseFolds = []\n",
    "for i in range(9):\n",
    "  spambaseFolds += [spambaseDf[i*foldSize:(i+1)*foldSize]]\n",
    "spambaseFolds += [spambaseDf[9*foldSize:len(spambaseDf)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatureSplitVar(df: pd.DataFrame, featureIndexesUsed: list) -> tuple: \n",
    "  \"\"\" finds best feature split for decision tree\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): dataframe to split\n",
    "      featureIndexesUsed (list): index of columns that were already used to split\n",
    "\n",
    "  Returns:\n",
    "      tuple: \n",
    "      - Tuple of split sides\n",
    "      - the index of columns that have been used to split\n",
    "      - split theta criteria\n",
    "  \"\"\"  \n",
    "  bestFeatureIndex = None\n",
    "  lowestVar = None\n",
    "  bestTheta = None\n",
    "  for i in range(len(df.columns) - 1):\n",
    "    if (i in featureIndexesUsed):\n",
    "      continue\n",
    "    for j in range(0, len(df)):\n",
    "      tempTheta = df.iloc[j, i]\n",
    "      left = df[df[i] <= tempTheta]\n",
    "      right = df[df[i] > tempTheta]\n",
    "      if len(left) > 1:\n",
    "        leftLabels = left[left.columns[-1]].to_list()\n",
    "        varLeft = np.var(leftLabels)        \n",
    "      else:\n",
    "        varLeft = 0\n",
    "      if len(right) > 1: \n",
    "        rightLabels = right[right.columns[-1]].to_list()\n",
    "        varRight = np.var(rightLabels)        \n",
    "      else:\n",
    "        varRight = 0\n",
    "      totalVar = ((len(left) / len(df)) * varLeft) + (len(right) / len(df)) * varRight\n",
    "      if lowestVar is None or totalVar < lowestVar:\n",
    "        bestTheta = tempTheta\n",
    "        lowestVar = totalVar\n",
    "        bestFeatureIndex = i\n",
    "  leftSplit: pd.DataFrame = df[df[bestFeatureIndex] <= bestTheta]\n",
    "  rightSplit: pd.DataFrame = df[df[bestFeatureIndex] > bestTheta]\n",
    "  featureIndexesUsed.append(bestFeatureIndex)\n",
    "  return (leftSplit, rightSplit), featureIndexesUsed, bestTheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n",
      "65\n",
      "<=>\n",
      "[5]\n",
      "6.968\n"
     ]
    }
   ],
   "source": [
    "splitDf, featureIndexesUsed, splitTheta = findFeatureSplitVar(trainingHouseDf, [])\n",
    "print(f\"{len(splitDf[0])}\")\n",
    "print(f\"{len(splitDf[1])}\")\n",
    "print(\"<=>\")\n",
    "print(featureIndexesUsed)\n",
    "print(splitTheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingVar(df: pd.DataFrame, featureIndexesUsed) -> tuple:  \n",
    "  if df[len(df.columns) - 1].nunique() == 1 or len(df) == 1:\n",
    "    return df.iloc[0, len(df.columns) - 1]\n",
    "  \n",
    "  if len(df) < 10 or len(featureIndexesUsed) == len(df.columns) -1:\n",
    "    return statistics.mean(df[len(df.columns) - 1])\n",
    "\n",
    "  splitDf, featureIndexesUsed, splitTheta = findFeatureSplitVar(df, featureIndexesUsed)\n",
    "\n",
    "  if len(splitDf[0]) == 0:\n",
    "    return statistics.mean(splitDf[1][len(df.columns) - 1])\n",
    "  elif len(splitDf[1]) == 0:\n",
    "    return statistics.mean(splitDf[0][len(df.columns) - 1])\n",
    "  else:\n",
    "    return (\n",
    "    (featureIndexesUsed[-1], splitTheta), \n",
    "    trainingVar(splitDf[0], featureIndexesUsed),\n",
    "    trainingVar(splitDf[1], featureIndexesUsed)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "((5, 0.9374831580810482), ((12, 0.36439674208479433), ((7, -1.1901742383672564), 45.58, ((10, 0.14829277385834794), ((9, -1.05543168557729), ((2, -1.2482952688616673), 30.0125, 35.25), ((1, 0.9891745186081486), ((8, -0.2792061735129452), ((6, 1.0609846384852255), ((4, -0.3716546836028746), ((0, -0.40139480653265436), ((11, 0.37545554157679106), 20.233333333333334, 22.583333333333332), ((3, -0.26792196292544174), 23.816279069767443, 22.549999999999997)), 21.313793103448276), 27.125), 27.333333333333336), 26.22)), 21.048514851485148)), 14.336641221374046), 38.25846153846154)\n"
     ]
    }
   ],
   "source": [
    "regularizedHouseTraining = trainingHouseDf.copy()\n",
    "regularizedHouseTraining.iloc[:, :-1] = regularizedHouseTraining.iloc[:, :-1].apply(zscore)\n",
    "housingDecisionTree = trainingVar(regularizedHouseTraining, [])\n",
    "print(\"Decision Tree:\")\n",
    "print(housingDecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatureSplitEntropy(df: pd.DataFrame, featureIndexesUsed: list) -> tuple: \n",
    "  \"\"\" finds best feature split for decision tree\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): dataframe to split\n",
    "      featureIndexesUsed (list): index of columns that were already used to split\n",
    "\n",
    "  Returns:\n",
    "      tuple: \n",
    "      - Tuple of split sides\n",
    "      - the index of columns that have been used to split\n",
    "      - split theta criteria\n",
    "  \"\"\"  \n",
    "  bestFeatureIndex = None\n",
    "  lowestEntropy = None\n",
    "  bestTheta = None\n",
    "  for i in range(len(df.columns) - 1):\n",
    "    if (i in featureIndexesUsed):\n",
    "      continue\n",
    "    for j in range(0, len(df), len(df)//25):\n",
    "      tempTheta = df.iloc[j, i]\n",
    "      left = df[df[i] <= tempTheta]\n",
    "      right = df[df[i] > tempTheta]\n",
    "      if len(left) > 1:\n",
    "        leftLabels = left[len(df.columns) - 1].to_list()\n",
    "        numLabels = len(leftLabels)\n",
    "        zeroProb = leftLabels.count(0)/numLabels\n",
    "        oneProb = leftLabels.count(1)/numLabels\n",
    "        entropyLeft = entropy([zeroProb, oneProb])\n",
    "      else:\n",
    "        entropyLeft = 0\n",
    "      if len(right) > 1: \n",
    "        rightLabels = right[len(df.columns) - 1].to_list()\n",
    "        numLabels = len(rightLabels)\n",
    "        zeroProb = rightLabels.count(0)/numLabels\n",
    "        oneProb = rightLabels.count(1)/numLabels\n",
    "        entropyRight = entropy([zeroProb, oneProb])\n",
    "      else:\n",
    "        entropyRight = 0\n",
    "      totalEntropy = ((len(left) / len(df)) * entropyLeft) + (len(right) / len(df)) * entropyRight\n",
    "      if lowestEntropy is None or totalEntropy < lowestEntropy:\n",
    "        bestTheta = tempTheta\n",
    "        lowestEntropy = totalEntropy\n",
    "        bestFeatureIndex = i\n",
    "  leftSplit: pd.DataFrame = df[df[bestFeatureIndex] <= bestTheta]\n",
    "  rightSplit: pd.DataFrame = df[df[bestFeatureIndex] > bestTheta]\n",
    "  featureIndexesUsed.append(bestFeatureIndex)\n",
    "  return (leftSplit, rightSplit), featureIndexesUsed, bestTheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingEntropy(df: pd.DataFrame, featureIndexesUsed) -> tuple:  \n",
    "  if df[len(df.columns) - 1].nunique() == 1 or len(df) == 1:\n",
    "    return df.iloc[0, len(df.columns) - 1]\n",
    "  \n",
    "  if len(df) < 500 or len(featureIndexesUsed) == len(df.columns) -1:\n",
    "    labels = df[len(df.columns) - 1].to_list()\n",
    "    return 0 if labels.count(0) >= labels.count(1) else 1\n",
    "\n",
    "  splitDf, featureIndexesUsed, splitTheta = findFeatureSplitEntropy(df, featureIndexesUsed)\n",
    "\n",
    "  if len(splitDf[0]) == 0:\n",
    "    labels = splitDf[1][len(splitDf[1].columns) - 1].to_list()\n",
    "    return 0 if labels.count(0) >= labels.count(1) else 1\n",
    "  elif len(splitDf[1]) == 0:\n",
    "    labels = splitDf[0][len(splitDf[0].columns) - 1].to_list()\n",
    "    return 0 if labels.count(0) >= labels.count(1) else 1\n",
    "  else:\n",
    "    return (\n",
    "    (featureIndexesUsed[-1], splitTheta), \n",
    "    trainingEntropy(splitDf[0], featureIndexesUsed),\n",
    "    trainingEntropy(splitDf[1], featureIndexesUsed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "((51, 0.05), ((6, 0.0), ((24, 0.0), ((55, 10), ((26, 0.0), ((10, 0.0), ((15, 0.0), ((18, 0.0), 0, 0), 0), 1), 0), 0), ((22, 0.0), ((11, 0.0), 0, 0), 0)), 1), ((52, 0.0), ((54, 3.0), ((25, 0.0), 0, 0), 1), ((36, 0.07), ((56, 742), ((5, 0.24), 1, 1), 1), 1)))\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "\n",
    "trainingSpam = pd.concat(spambaseFolds[:9])\n",
    "spambaseDecisionTree = trainingEntropy(trainingSpam, [])\n",
    "print(\"===\")\n",
    "print(spambaseDecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(testPoint: list, tree):\n",
    "  if type(tree) is tuple:\n",
    "    return testing(testPoint, tree[1]) if testPoint[tree[0][0]] <= tree[0][1] else testing(testPoint, tree[2])\n",
    "  else:\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean square error\n",
    "def calcMSE(yPred: list, yActual: list) -> float:\n",
    "    squaredErrors = []\n",
    "    for i in range(len(yPred)):\n",
    "        squaredErrors.append((yPred[i] - yActual[i])**2)\n",
    "    mse = sum(squaredErrors) / len(yPred)\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 22.595557928241938\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(len(regularizedHouseTraining)):\n",
    "  testingPoint = regularizedHouseTraining.iloc[i].to_list()\n",
    "  prediction = testing(testingPoint[:-1], housingDecisionTree)\n",
    "  predictions.append(prediction)\n",
    "\n",
    "y_true = trainingHouseDf.iloc[:,-1].to_list()\n",
    "print(f\"Training MSE: {calcMSE(predictions, y_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MSE: 31.380507827725733\n"
     ]
    }
   ],
   "source": [
    "regularIzedHouseTesting = testingHouseDf.copy()\n",
    "regularIzedHouseTesting.iloc[:, :-1] = regularIzedHouseTesting.iloc[:, :-1].apply(zscore)\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(regularIzedHouseTesting)):\n",
    "  testingPoint = regularIzedHouseTesting.iloc[i].to_list()\n",
    "  prediction = testing(testingPoint[:-1], housingDecisionTree)\n",
    "  predictions.append(prediction)\n",
    "\n",
    "y_true = testingHouseDf.iloc[:,-1].to_list()\n",
    "print(f\"Testing MSE: {calcMSE(predictions, y_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# test cell\n",
    "\n",
    "testing(spambaseFolds[9][0].to_list(), spambaseDecisionTree)\n",
    "print(spambaseFolds[9][0].to_list()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan Yu\\AppData\\Local\\Temp\\ipykernel_12152\\1018614307.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.iloc[:, :-1] = test.iloc[:, :-1].apply(zscore)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.3744128935693145\n",
      "Testing Error: 0.36695652173913046\n"
     ]
    }
   ],
   "source": [
    "# k-fold validation full code\n",
    "\n",
    "trainingErrorRates = []\n",
    "errorRates = []\n",
    "for k in range(10):\n",
    "  trainingWrongCount = 0\n",
    "  wrongCount = 0\n",
    "  trainingData = spambaseFolds[:k] + spambaseFolds[k+1:]\n",
    "  test: pd.DataFrame = spambaseFolds[k]\n",
    "  training = pd.concat(trainingData)\n",
    "  training.iloc[:, :-1] = training.iloc[:, :-1].apply(zscore)\n",
    "  test.iloc[:, :-1] = test.iloc[:, :-1].apply(zscore)\n",
    "  decisionTree = trainingEntropy(training, [])\n",
    "\n",
    "  for i in range(len(training)):\n",
    "    testingPoint = training.iloc[i].to_list()\n",
    "    prediction = testing(testingPoint[:-1], decisionTree)\n",
    "    if prediction != testingPoint[-1]:\n",
    "      trainingWrongCount += 1\n",
    "  trainingErrorRates.append(trainingWrongCount/len(training))\n",
    "\n",
    "  for i in range(len(test)):\n",
    "    testingPoint = test.iloc[i].to_list()\n",
    "    prediction = testing(testingPoint[:-1], decisionTree)\n",
    "    if prediction != testingPoint[-1]:\n",
    "      wrongCount += 1\n",
    "  errorRates.append(wrongCount/len(test))\n",
    "\n",
    "print(f\"Training Error: {statistics.mean(trainingErrorRates)}\")\n",
    "print(f\"Testing Error: {statistics.mean(errorRates)}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM 2 [50 points]**\n",
    "\n",
    "Using each of the two datasets above, apply regression on the training set to find a linear fit with the labels.  Implement linear algebra exact solution (normal equations).\n",
    "* Compare the training and testing errors (mean sum of square differences between prediction and actual label).\n",
    "* Compare with the decision tree results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-import\n",
    "\n",
    "trainingHouseDf = pd.read_csv(r\"housing_train.txt\", header=None, delim_whitespace=True)\n",
    "testingHouseDf = pd.read_csv(r\"housing_test.txt\", header=None, delim_whitespace=True)\n",
    "\n",
    "spambaseDf = pd.read_csv(r\"spambase.data\", header=None)\n",
    "\n",
    "foldSize = int(len(spambaseDf)/10)\n",
    "spambaseFolds = []\n",
    "for i in range(9):\n",
    "  spambaseFolds += [spambaseDf[i*foldSize:(i+1)*foldSize]]\n",
    "spambaseFolds += [spambaseDf[9*foldSize:len(spambaseDf)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addIntercept(matrix: np.array) -> np.array:\n",
    "  x = matrix.copy()\n",
    "  x.insert(0, \"intercept\", [1 for i in range(len(x))])\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   intercept        0     1     2  3      4      5     6       7  8      9  \\\n",
      "0          1  0.00632  18.0  2.31  0  0.538  6.575  65.2  4.0900  1  296.0   \n",
      "1          1  0.02731   0.0  7.07  0  0.469  6.421  78.9  4.9671  2  242.0   \n",
      "2          1  0.02729   0.0  7.07  0  0.469  7.185  61.1  4.9671  2  242.0   \n",
      "3          1  0.03237   0.0  2.18  0  0.458  6.998  45.8  6.0622  3  222.0   \n",
      "4          1  0.06905   0.0  2.18  0  0.458  7.147  54.2  6.0622  3  222.0   \n",
      "\n",
      "     10      11    12    13  \n",
      "0  15.3  396.90  4.98  24.0  \n",
      "1  17.8  396.90  9.14  21.6  \n",
      "2  17.8  392.83  4.03  34.7  \n",
      "3  18.7  394.63  2.94  33.4  \n",
      "4  18.7  396.90  5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "x = addIntercept(trainingHouseDf)\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24.0\n",
      "1    21.6\n",
      "2    34.7\n",
      "3    33.4\n",
      "4    36.2\n",
      "Name: 13, dtype: float64\n",
      "   intercept        0     1     2  3      4      5     6       7  8      9  \\\n",
      "0          1  0.00632  18.0  2.31  0  0.538  6.575  65.2  4.0900  1  296.0   \n",
      "1          1  0.02731   0.0  7.07  0  0.469  6.421  78.9  4.9671  2  242.0   \n",
      "2          1  0.02729   0.0  7.07  0  0.469  7.185  61.1  4.9671  2  242.0   \n",
      "3          1  0.03237   0.0  2.18  0  0.458  6.998  45.8  6.0622  3  222.0   \n",
      "4          1  0.06905   0.0  2.18  0  0.458  7.147  54.2  6.0622  3  222.0   \n",
      "\n",
      "     10      11    12  \n",
      "0  15.3  396.90  4.98  \n",
      "1  17.8  396.90  9.14  \n",
      "2  17.8  392.83  4.03  \n",
      "3  18.7  394.63  2.94  \n",
      "4  18.7  396.90  5.33  \n"
     ]
    }
   ],
   "source": [
    "housingLabels = x[x.columns[len(x.columns)-1]]\n",
    "training = x.drop(columns=x.columns[-1], axis=1)\n",
    "print(housingLabels.head())\n",
    "print(training.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433, 14)\n",
      "(433,)\n"
     ]
    }
   ],
   "source": [
    "trainingMatrix = training.to_numpy()\n",
    "labelsMatrix = housingLabels.to_numpy()\n",
    "print(trainingMatrix.shape)\n",
    "print(labelsMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getW(x: np.array, y: np.array) -> np.array:\n",
    "  xTranspose = np.transpose(x)\n",
    "  dxdInverse = np.linalg.inv(np.matmul(xTranspose, x))\n",
    "  return np.matmul(dxdInverse, np.matmul(xTranspose, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.95843212e+01 -1.01137046e-01  4.58935299e-02 -2.73038670e-03\n",
      "  3.07201340e+00 -1.72254072e+01  3.71125235e+00  7.15862492e-03\n",
      " -1.59900210e+00  3.73623375e-01 -1.57564197e-02 -1.02417703e+00\n",
      "  9.69321451e-03 -5.85969273e-01]\n"
     ]
    }
   ],
   "source": [
    "w = getW(trainingMatrix, labelsMatrix)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error: 22.081273187013167\n"
     ]
    }
   ],
   "source": [
    "# training predictions\n",
    "\n",
    "trainPredictions = np.matmul(trainingMatrix, w)\n",
    "print(f\"Testing Error: {calcMSE(trainPredictions, labelsMatrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   intercept        0    1     2  3      4      5     6       7  8      9  \\\n",
      "0          1  0.84054  0.0  8.14  0  0.538  5.599  85.7  4.4546  4  307.0   \n",
      "1          1  0.67191  0.0  8.14  0  0.538  5.813  90.3  4.6820  4  307.0   \n",
      "2          1  0.95577  0.0  8.14  0  0.538  6.047  88.8  4.4534  4  307.0   \n",
      "3          1  0.77299  0.0  8.14  0  0.538  6.495  94.4  4.4547  4  307.0   \n",
      "4          1  1.00245  0.0  8.14  0  0.538  6.674  87.3  4.2390  4  307.0   \n",
      "\n",
      "     10      11     12  \n",
      "0  21.0  303.42  16.51  \n",
      "1  21.0  376.88  14.81  \n",
      "2  21.0  306.38  17.28  \n",
      "3  21.0  387.94  12.80  \n",
      "4  21.0  380.23  11.98  \n",
      "0    13.9\n",
      "1    16.6\n",
      "2    14.8\n",
      "3    18.4\n",
      "4    21.0\n",
      "Name: 13, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "housingTesting = addIntercept(testingHouseDf)\n",
    "\n",
    "testLabels = housingTesting[housingTesting.columns[len(housingTesting.columns)-1]]\n",
    "testFeatures = housingTesting.drop(columns=housingTesting.columns[-1], axis=1)\n",
    "print(testFeatures.head())\n",
    "print(testLabels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 14)\n",
      "(74,)\n"
     ]
    }
   ],
   "source": [
    "testingMatrix = testFeatures.to_numpy()\n",
    "testLabelsMatrix = testLabels.to_numpy()\n",
    "print(testingMatrix.shape)\n",
    "print(testLabelsMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error: 22.63825629658623\n"
     ]
    }
   ],
   "source": [
    "testPredictions = np.matmul(testingMatrix, w)\n",
    "print(f\"Testing Error: {calcMSE(testPredictions, testLabelsMatrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.10558060843199908\n",
      "Testing Error: 0.15062670942186174\n"
     ]
    }
   ],
   "source": [
    "trainErrors = []\n",
    "testErrors = []\n",
    "\n",
    "for k in range(10):\n",
    "  trainingData = spambaseFolds[:k] + spambaseFolds[k+1:]\n",
    "  test: pd.DataFrame = addIntercept(spambaseFolds[k])\n",
    "  training = addIntercept(pd.concat(trainingData))\n",
    "  \n",
    "  trainingLabels = training[training.columns[len(training.columns)-1]]\n",
    "  trainingFeatures = training.drop(columns=training.columns[-1], axis=1)\n",
    "\n",
    "  trainingMatrix = trainingFeatures.to_numpy()\n",
    "  trainingLabelsMatrix = trainingLabels.to_numpy()\n",
    "  w = getW(trainingMatrix, trainingLabelsMatrix)\n",
    "  trainResults = np.matmul(trainingMatrix, w)\n",
    "  trainPredictions = list(map(lambda x: 0 if x <= 0.5 else 1, trainResults))\n",
    "  wrongCount = 0\n",
    "  for i in range(len(trainPredictions)):\n",
    "    if trainPredictions[i] != trainingLabelsMatrix[i]:\n",
    "      wrongCount += 1\n",
    "  trainErrors.append(wrongCount/len(trainPredictions))\n",
    "\n",
    "  testingLabels = test[test.columns[len(test.columns)-1]]\n",
    "  testingFeatures = test.drop(columns=test.columns[-1], axis=1)\n",
    "\n",
    "  testingMatrix = testingFeatures.to_numpy()\n",
    "  testingLabelsMatrix = testingLabels.to_numpy()\n",
    "  testingResults = np.matmul(testingMatrix, w)\n",
    "  testPredictions = list(map(lambda x: 0 if x <= 0.5 else 1, testingResults))\n",
    "  wrongCount = 0\n",
    "  for i in range(len(testPredictions)):\n",
    "    if testPredictions[i] != testingLabelsMatrix[i]:\n",
    "      wrongCount += 1\n",
    "  testErrors.append(wrongCount/len(testPredictions))\n",
    "\n",
    "print(f\"Training Error: {statistics.mean(trainErrors)}\")\n",
    "print(f\"Testing Error: {statistics.mean(testErrors)}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM 3 [20 points]**\n",
    "\n",
    "DHS chapter8, Pb1. Given an arbitrary decision tree, it might have repeated queries splits (feature f, threshold t) on some paths root-leaf. Prove that there exists an equivalent decision tree only with distinct splits on each path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are two splits with the same feature and threshold on the same path,  second split will have one side have all of the data and the other side will have none. In training, there will be no reduction of variance or entropy reduction when a split has all of the data on one side and none on the other. Therefore, removing the duplicate splits will keep the same variance reductions at each node with the same paths leaving an equivalent decision tree only with distinct splits on each path. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM 4 [20 points]**\n",
    "\n",
    "DHS chapter8,\n",
    "Consider a binary decision tree using entropy splits.\n",
    "<ol type=\"a\">\n",
    "  <li> Prove that the decrease in entropy by a split on a binary yes/no feature can never be greater than 1 bit.\n",
    "  <li> Generalize this result to the case  of arbitrary branching  B>1.\n",
    "</ol>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: The range that entropy is measured is between 1 and 0 where 1 is the highest amount of entropy and 0 is the lowest amount of entropy. By definition, a perfect split would result in both sides have 0 entropy and would therefore sum to 0. Therefore, the highest decrease of entropy that can happen is 1 - 0, which is 1. \n",
    "\n",
    "b: The maximum amount of entropy of the node before the split is still 1. A perfect split will result in 0 entropy for every branch. When summed together, the entropy will be 0. 1 - 0 is one, so the decrease in entropy is still maximum 1 bit. Any imperfect split would result in an entropy > 0 which would make the decrease less than 1. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM 5 [20 points]**\n",
    "\n",
    "Derive explicit formulas for normal equations solution presented in class for the case of one input dimension.\n",
    "(Essentially assume the data is (x_i,y_i) i=1,2,..., m and you are looking for h(x) = ax+b that realizes the minimum mean square error. The problem asks you to write down explicit formulas for a and b.)\n",
    "\n",
    "HINT: Do not simply copy the formulas from [here](http://mathworld.wolfram.com/LeastSquaresFitting.html) (but do read the article): either take the general formula derived in class and make the calculations (inverse, multiplications, transpose) for one dimension or derive the formulas for a and b from scratch; in either case show the derivations. You can compare your end formulas with the ones linked above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Problem 5\" alt=\"\" src=\"./Problem 5.jpg\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM 7 [20points]**\n",
    "\n",
    "DHS chapter5,\n",
    "The convex hull of a set of vectors xi,i = 1,...,n is the set of all vectors of the form <br>\n",
    "<img title=\"image\" alt=\"image\" src=\"https://www.ccs.neu.edu/home/vip/teach/MLcourse/1_intro_DT_RULES_REG/hw1/latex-image-1.jpeg\"> <br>\n",
    "where the coefficients Î±i are nonnegative and sum to one. Given two sets of vectors, show that either they are linearly separable or their convex hulls intersect.\n",
    "Hint on easy part: that the two conditions cannot happen simultaneously. Suppose that both statements are true, and consider the classification of a point in the intersection of the convex hulls.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Problem 7\" alt=\"\" src=\"./Problem 7.jpg\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2ec967f60ddb5fee6ccddb8c3e31b29fda35ec14560f386d977bd743bed2709"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
